#!/usr/bin/env python3
"""
ë³‘ë¬´ì²­ ìœ¡êµ° ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ - GitHub Actions ë²„ì „
"""
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date
import time
import logging
from typing import List, Dict, Optional

from text_summarizer import SimpleTextSummarizer
from email_sender import EmailSender

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MMABoardCrawler:
    def __init__(self):
        self.base_url = "https://www.mma.go.kr"
        self.board_url = "/board/boardList.do?gesipan_id=69&mc=usr0000127"
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.summarizer = SimpleTextSummarizer(max_length=300)
        self.email_sender = EmailSender()
    
    def get_latest_posts(self, count: int = 1) -> List[Dict]:
        """
        ìµœì‹  ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ (ìˆ˜ë™ í…ŒìŠ¤íŠ¸ìš©)
        """
        posts = []
        
        try:
            logger.info(f"ğŸ” ìµœì‹  ê²Œì‹œê¸€ {count}ê°œ í¬ë¡¤ë§: {self.base_url + self.board_url}")
            
            response = self.session.get(
                self.base_url + self.board_url, 
                timeout=30
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ í…Œì´ë¸” ì°¾ê¸°
            table = soup.select_one('table')
            if not table:
                logger.warning("âš ï¸  ê²Œì‹œê¸€ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return posts
            
            # í…Œì´ë¸” í–‰ ì¶”ì¶œ
            rows = table.select('tbody tr')
            logger.info(f"ğŸ“„ ì´ {len(rows)}ê°œ í–‰ ë°œê²¬")
            
            for idx, row in enumerate(rows[:count]):  # ìµœì‹  Nê°œë§Œ
                cells = row.select('td')
                
                logger.debug(f"í–‰ {idx}: ì…€ ê°œìˆ˜ = {len(cells)}")
                
                # ìµœì†Œ 4ê°œ ì…€ì´ ìˆì–´ì•¼ í•¨ (ì œëª©, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                if len(cells) < 4:
                    logger.debug(f"í–‰ {idx}: ì…€ ë¶€ì¡± (ìµœì†Œ 4ê°œ í•„ìš”)")
                    continue
                
                try:
                    # ê° ì…€ì˜ ë‚´ìš© ë¡œê¹…
                    for i, cell in enumerate(cells[:4]):
                        logger.debug(f"  ì…€ {i}: {cell.get_text(strip=True)[:30]}")
                    
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì…€)
                    title_cell = cells[0]
                    title_link = title_cell.select_one('a')
                    
                    if title_link and title_link.get('href'):
                        post = {
                            'title': title_link.get_text(strip=True),
                            'url': self._build_full_url(title_link.get('href')),
                            'date': cells[2].get_text(strip=True),  # ì„¸ ë²ˆì§¸ ì…€ì´ ë‚ ì§œ
                            'number': str(idx + 1)  # ë²ˆí˜¸ëŠ” ì¸ë±ìŠ¤ë¡œ ëŒ€ì²´
                        }
                        posts.append(post)
                        logger.info(f"âœ… ê²Œì‹œê¸€ ë°œê²¬: {post['title']}")
                    else:
                        logger.debug(f"í–‰ {idx}: ë§í¬ ì—†ìŒ")
                
                except (ValueError, AttributeError) as e:
                    logger.debug(f"í–‰ {idx}: íŒŒì‹± ì˜¤ë¥˜ - {e}")
                    continue
            
            logger.info(f"ğŸ¯ ìµœì‹  ê²Œì‹œê¸€ {len(posts)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return posts
    
    def get_today_posts(self) -> List[Dict]:
        """
        ì˜¤ëŠ˜ ì‘ì„±ëœ ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ
        """
        posts = []
        today = date.today()
        
        try:
            logger.info(f"ğŸ” ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘: {self.base_url + self.board_url}")
            
            response = self.session.get(
                self.base_url + self.board_url, 
                timeout=30
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ í…Œì´ë¸” ì°¾ê¸°
            table = soup.select_one('table')
            if not table:
                logger.warning("âš ï¸  ê²Œì‹œê¸€ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return posts
            
            # í…Œì´ë¸” í–‰ ì¶”ì¶œ
            rows = table.select('tbody tr')
            logger.info(f"ğŸ“„ ì´ {len(rows)}ê°œ í–‰ ë°œê²¬")
            
            for row_idx, row in enumerate(rows):
                cells = row.select('td')
                
                # ìµœì†Œ 4ê°œ ì…€ì´ ìˆì–´ì•¼ í•¨ (ì œëª©, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                if len(cells) < 4:
                    continue
                
                try:
                    # ì‘ì„±ì¼ ì¶”ì¶œ ë° íŒŒì‹± (ì„¸ ë²ˆì§¸ ì…€)
                    date_text = cells[2].get_text(strip=True)
                    
                    # ë‚ ì§œ í˜•ì‹ í™•ì¸ (YYYY-MM-DD)
                    post_date = datetime.strptime(date_text, '%Y-%m-%d').date()
                    
                    # ì˜¤ëŠ˜ ë‚ ì§œì™€ ë¹„êµ
                    if post_date == today:
                        # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì…€)
                        title_cell = cells[0]
                        title_link = title_cell.select_one('a')
                        
                        if title_link and title_link.get('href'):
                            post = {
                                'title': title_link.get_text(strip=True),
                                'url': self._build_full_url(title_link.get('href')),
                                'date': date_text,
                                'number': str(row_idx + 1)
                            }
                            posts.append(post)
                            logger.info(f"âœ… ìƒˆ ê²Œì‹œê¸€ ë°œê²¬: {post['title']}")
                
                except (ValueError, AttributeError) as e:
                    # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë§í¬ ì—†ìŒ - ê³„ì† ì§„í–‰
                    continue
            
            logger.info(f"ğŸ¯ ì˜¤ëŠ˜ ì‘ì„±ëœ ê²Œì‹œê¸€: {len(posts)}ê±´")
            
        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return posts
    
    def get_post_content(self, post_url: str) -> Optional[str]:
        """
        ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
        """
        try:
            logger.info(f"ğŸ“– ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§: {post_url}")
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(1)
            
            response = self.session.get(post_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            content_selectors = [
                'table tbody tr td',  # ê¸°ë³¸ í…Œì´ë¸” êµ¬ì¡°
                '.board-content',      # í´ë˜ìŠ¤ëª… ê¸°ë°˜
                '#content',            # ID ê¸°ë°˜
                'div.content',         # div ì»¨í…Œì´ë„ˆ
            ]
            
            content = None
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(separator='\n', strip=True)
                    if len(text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸
                        content = text
                        break
                if content:
                    break
            
            if content:
                logger.info(f"âœ… ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: {len(content)}ì")
                return content
            else:
                logger.warning("âš ï¸  ê²Œì‹œê¸€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_full_url(self, relative_url: str) -> str:
        """ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜"""
        if relative_url.startswith('http'):
            return relative_url
        elif relative_url.startswith('/'):
            return self.base_url + relative_url
        else:
            # ìƒëŒ€ URLì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì¶”ê°€
            if 'boardView.do' in relative_url and 'pageIndex=' not in relative_url:
                # pageIndexì™€ ê¸°íƒ€ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì¶”ê°€
                if '?' in relative_url:
                    relative_url += '&pageIndex=1&searchCondition=&searchKeyword=&pageUnit=10&mc=usr0000127&jbc_gonggibodo=0'
                else:
                    relative_url += '?pageIndex=1&searchCondition=&searchKeyword=&pageUnit=10&mc=usr0000127&jbc_gonggibodo=0'
            
            return self.base_url + '/board/' + relative_url
    
    def process_posts(self, posts: List[Dict]) -> List[Dict]:
        """
        ê²Œì‹œê¸€ ëª©ë¡ ì²˜ë¦¬ (ë‚´ìš© í¬ë¡¤ë§ ë° ìš”ì•½)
        """
        processed_posts = []
        
        for post in posts:
            logger.info(f"ğŸ”„ ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘: {post['title']}")
            
            # ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§
            content = self.get_post_content(post['url'])
            
            # ë‚´ìš© ìš”ì•½
            if content:
                summary = self.summarizer.summarize(content)
                post['summary'] = summary
                post['content_length'] = len(content)
            else:
                post['summary'] = "ê²Œì‹œê¸€ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                post['content_length'] = 0
            
            processed_posts.append(post)
        
        return processed_posts
    
    def run(self):
        """
        í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
        """
        logger.info("ğŸš€ ë³‘ë¬´ì²­ ìœ¡êµ° ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ìˆ˜ë™ ì‹¤í–‰ ëª¨ë“œ í™•ì¸ (GitHub Actions workflow_dispatch)
        is_manual = os.getenv('MANUAL_MODE', 'false').lower() == 'true'
        
        try:
            if is_manual:
                # ìˆ˜ë™ ì‹¤í–‰: ìµœì‹  ê²Œì‹œê¸€ 1ê°œ
                logger.info("ğŸ”§ ìˆ˜ë™ ì‹¤í–‰ ëª¨ë“œ: ìµœì‹  ê²Œì‹œê¸€ 1ê°œ ì¡°íšŒ")
                posts = self.get_latest_posts(1)
                
                if not posts:
                    logger.info("âŒ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
            else:
                # ìë™ ì‹¤í–‰: ì˜¤ëŠ˜ ì‘ì„±ëœ ê²Œì‹œê¸€
                logger.info("â° ìë™ ì‹¤í–‰ ëª¨ë“œ: ì˜¤ëŠ˜ ì‘ì„±ëœ ê²Œì‹œê¸€ ì¡°íšŒ")
                posts = self.get_today_posts()
                
                if not posts:
                    logger.info("â„¹ï¸  ì˜¤ëŠ˜ ì‘ì„±ëœ ìƒˆ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return
            
            # 2. ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§ ë° ìš”ì•½
            processed_posts = self.process_posts(posts)
            
            # 3. ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡
            success = self.email_sender.send_notification(processed_posts)
            
            if success:
                logger.info("ğŸ‰ í¬ë¡¤ë§ ë° ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ!")
            else:
                logger.error("âŒ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        crawler = MMABoardCrawler()
        crawler.run()
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()